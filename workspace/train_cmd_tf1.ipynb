{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7764ee9-5abf-48d7-865b-30d10f1fe7c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This notebook will help you go through the below steps (1). model setting (2). training (3). exporting tflite\n",
    "---\n",
    "- The `training_demo` is user defined folder name under `workspace`. You can check `image_dataset\\create_data.ipynb` for how to create your own training folder.\n",
    "- In this notebook step, you have alreay finish the dataset prepared. If not, please go to `image_dataset\\create_data.ipynb`.\n",
    "- It is recommended to copy the cmds below and use CMD, PowerShell or terminal outside this notebook.\n",
    "- All the commands below are needed excuted under `workspace\\training_demo`.\n",
    "- \\<Advanced>: The more detail is in this link [tensorflow-object-detection-api-tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f248ab-8c0d-4dca-9411-ad6e5676264f",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa05feb-96e9-4fb7-8622-930a8f8df3bd",
   "metadata": {},
   "source": [
    "## Download Pre-Trained Model\n",
    "- The model in this examples is the `ssd_mobilenet_v3_small_coco`\n",
    "- All of the tensorflow1 pre-trained models are listed in [TensorFlow 1 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md), and you can choose other models and download it.\n",
    "- The download file is `*.tar.gz`, and please decompression it (e.g. 7zip, WinZIP, etc.).\n",
    "- Move `ssd_mobilenet_v3_small_coco_2020_01_14` inside the folder `training_demo/pre-trained-models`\n",
    "\n",
    "- <pre>training_demo/\n",
    "├─ ...\n",
    "├─ pre-trained-models/\n",
    "│  └─ ssd_mobilenet_v3_small_coco_2020_01_14/\n",
    "│     ├─ checkpoint\n",
    "│     ├─ frozen_inference_graph.pb\n",
    "│     ├─ pipeline.config\n",
    "│     └─ ... \n",
    "└─ ...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d06f6-3e5e-48f0-91d3-8a9552447d1b",
   "metadata": {},
   "source": [
    "## Configure the Training Pipeline\n",
    "- The parameters below is basing on your files/folders nameing. Please update them if any change.\n",
    "    1. `training_dir`: The folder name of user defined working directory\n",
    "    2. `my_model_directory_name`: The file location of user defined which save the training weights, checkpoints and *.config\n",
    "    3. `fine_tune_checkpoint`: The file location of user downloaded pre-trained-models checkpoint\n",
    "    4. `train_record_fname`: The file location of user created tfrecord for training\n",
    "    5. `test_record_fname`: The file location of user created tfrecord for testing\n",
    "    6. `label_map_pbtxt_fname`: The file location of label map\n",
    "    7. `batch_size`: Increase/Decrease this value depending on the available memory\n",
    "    8. `num_steps`: How many the training steps.  \n",
    "- Please excute the below 2 blocks.\n",
    "- This is for `ssd_mobilenet_v3_small_coco` pipeline.config, if you use other model, the pipeline.config maybe have minor different. However, these attributes should be the same and mattered.\n",
    "- \\<Advanced>: If you want to tunning more parameters, please update `pipeline.config` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b962f4-5f91-491d-87c0-0c2e2a4a051f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dir = 'training_demo_tf1'\n",
    "\n",
    "my_model_directory_name = 'models/my_ssd_mobilenet_v3' \n",
    "fine_tune_checkpoint = 'pre-trained-models/ssd_mobilenet_v3_small_coco_2020_01_14/model.ckpt' \n",
    "train_record_fname = 'annotations/train.record' \n",
    "test_record_fname = 'annotations/test.record' \n",
    "label_map_pbtxt_fname = \"annotations/label_map.pbtxt\" \n",
    "batch_size = 32 \n",
    "num_steps = 80000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c3364e-611d-4056-9be1-93e08ea3a2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of class from label_map.pbtxt: 2\n",
      "writing custom configuration file...\n",
      "The train config file is at: C:\\Users\\USER\\Desktop\\ML_tf2_object_detection_nu\\workspace\\training_demo_tf1\\models/my_ssd_mobilenet_v3\\pipeline.config\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import regex as re\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "home_path = os.getcwd() \n",
    "path_para_list = [my_model_directory_name, fine_tune_checkpoint, train_record_fname, test_record_fname, label_map_pbtxt_fname]\n",
    "update_path_para_list = list(map(lambda x : os.path.join(home_path, training_dir, x), path_para_list))  #update the\n",
    "\n",
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    \n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())\n",
    "num_classes = get_num_classes(update_path_para_list[4])\n",
    "print('The number of class from label_map.pbtxt: {}'.format(num_classes))\n",
    "\n",
    "def create_user_folder(dir_path):\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "        print('skip create...')\n",
    "def copy_user_file(src, dst):\n",
    "    try:\n",
    "        shutil.copy(src, dst)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "def update_config(src_fld, dst_fld):\n",
    "    print('writing custom configuration file...')\n",
    "\n",
    "    with open(os.path.join(src_fld, 'pipeline.config')) as f:\n",
    "        s = f.read()\n",
    "    print('The train config file is at: {}'.format(os.path.join(dst_fld, 'pipeline.config')))    \n",
    "    with open(os.path.join(dst_fld, 'pipeline.config'), 'w') as f:\n",
    "                \n",
    "        # label_map_path\n",
    "        s = re.sub(\n",
    "            'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "        # Set training batch_size.\n",
    "        s = re.sub('batch_size: [0-9]+',\n",
    "                   'batch_size: {}'.format(batch_size), s)\n",
    "        \n",
    "        # Set training steps, num_steps\n",
    "        if(not re.search('fine_tune_checkpoint: \".*?\"', s)):\n",
    "            s = re.sub('num_steps: [0-9]+',\n",
    "                   'num_steps: {}\\n  fine_tune_checkpoint: \"{}\"'.format(num_steps, fine_tune_checkpoint), s)\n",
    "        else:\n",
    "            s = re.sub('num_steps: [0-9]+',\n",
    "                   'num_steps: {}'.format(num_steps), s)\n",
    "        # fine_tune_checkpoint\n",
    "            s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                   'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "        \n",
    "        # Set number of classes num_classes.\n",
    "        s = re.sub('num_classes: [0-9]+',\n",
    "                   'num_classes: {}'.format(num_classes), s)\n",
    "        #fine-tune checkpoint type\n",
    "        s = re.sub(\n",
    "            'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "        \n",
    "        # tfrecord files train and test. (the train section must before test section)\n",
    "        s = re.sub(\n",
    "            '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s, 1)\n",
    "        s = re.sub(\n",
    "            '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s, 1)\n",
    "        \n",
    "        f.write(s)            \n",
    "# create model_directory            \n",
    "create_user_folder(update_path_para_list[0])\n",
    "# copy pipeline.config\n",
    "update_config(update_path_para_list[1].split(r'model.ckpt')[-2], update_path_para_list[0])\n",
    "#copy_user_file(os.path.join(update_path_para_list[1].split(r'checkpoint')[-2], 'pipeline.config'), update_path_para_list[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57683d68-ee92-4359-a772-d7d73de7f1d7",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "- Please open CMD.exe Prompt or  PowerShell Prompt and `cd` inside your working folder, for example, `training_demo_tf1` folder.\n",
    "    - for example: `cd ML_tf2_object_detection_nu\\workspace\\training_demo_tf1`\n",
    "- Train Commands Help:\n",
    "    - `--model_dir` is user defined folder which is user defined my_model_directory_name. The training processes and variables are saved in here.\n",
    "    - `--pipeline_config_path` is the location of user defined pipeline.config.\n",
    "- <pre> training_demo/\n",
    "├─ ...\n",
    "├─ models/\n",
    "│  └─ my_ssd_mobilenet_v3/\n",
    "│     └─ pipeline.config\n",
    "└─ ...\n",
    "</pre>\n",
    "- The output will normally look like it has “frozen”, but DO NOT rush to cancel the process. The training outputs logs only every 100 steps by default, therefore if you wait for a while, you should see a log for the loss at step 100.\n",
    "\n",
    "\n",
    "- <img src=\"train_exmple_plots/train_process_tf1.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "- Tf1 version will run evaluating at the same time. If there is an error, please check [help](#id-IH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04304c0f-f422-4c54-a67e-2ab5c0a8a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python model_main.py --model_dir=models/my_ssd_mobilenet_v3 --pipeline_config_path=models/my_ssd_mobilenet_v3/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b526619-caef-4e12-b7dd-6ca8a246afd4",
   "metadata": {},
   "source": [
    "# Evaluating the Model (Optional)\n",
    "---\n",
    "- The tf1 version will run evaluating when training, so no need to use this command again.\n",
    "- Please open another CMD.exe Prompt or PowerShell Prompt to run the command when running the train step.\n",
    "- The operation step is same as training the model, and you need inside your working folder to excute the command.\n",
    "    - `--checkpoint_dir` is the location of each training save point, and it is saved in models folder.\n",
    "- If there is an error, please check [help](#id-IH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ba88c-41a2-4b8b-b32b-247692b80870",
   "metadata": {},
   "outputs": [],
   "source": [
    "python model_main.py --model_dir=models/my_ssd_mobilenet_v3 --pipeline_config_path=models/my_ssd_mobilenet_v3/pipeline.config --checkpoint_dir=models/my_ssd_mobilenet_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30aa668-6853-4bcb-8a82-4505204f203a",
   "metadata": {},
   "source": [
    "# Monitor Training Job Progress using TensorBoard\n",
    "---\n",
    "- Please open another CMD.exe Prompt or PowerShell Prompt to run the command when running the train step.\n",
    "- You need inside your working folder to excute the command.\n",
    "    - `--logdir` is the location of each training save point, and it is saved in models folder.\n",
    "- Copy the URL and paste it on browser (except IE) as below:\n",
    "- <img src=\"train_exmple_plots/tf_board_url.png\" width=\"400\" height=\"300\">\n",
    "- The board is as below:\n",
    "- <img src=\"train_exmple_plots/tf_board_tf1.png\" width=\"400\" height=\"300\">\n",
    "- In tf1 version, there is a chance that can't open TensorBoard, please check [help](#id-IH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cff18-289c-463c-a40a-9f1c1189e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=models/my_ssd_mobilenet_v3 --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826940b8-72c0-4a2d-93b9-a9ad0a7988e3",
   "metadata": {},
   "source": [
    "# Export a TFLite inference graph\n",
    "---\n",
    " To deploy on edge device, we should use this command (output TFLite inference graph).\n",
    "- Please open another CMD.exe Prompt or PowerShell Prompt to run the command.\n",
    "- The operation step is same as training the model, and you need inside your working folder to excute the command.\n",
    "    - `--pipeline_config_path` is the location of user defined pipeline.config.\n",
    "    - `--trained_checkpoint_prefix` is the location of each training save point, and it is saved in models folder. Please check the `models/` to update `.../model.ckpt-xxxx` the location and ckpt's step.\n",
    "    - `--output_directory` is the user defined folder to save your output model graph, for example `tflite_infer_graph_XX`. In this way, it is easy to distinguish different model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba1dd1-1d86-41eb-b8eb-253918b9b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "python export_tflite_ssd_graph.py --pipeline_config_path models/my_ssd_mobilenet_v3/pipeline.config --trained_checkpoint_prefix models/my_ssd_mobilenet_v3/model.ckpt-568 --output_directory exported-models/inference_graph_tflite --add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab721a2-adad-45fc-889e-c164ca7a3508",
   "metadata": {},
   "source": [
    "# Export a Trained Model graph (Optional)\n",
    "---\n",
    "- Output the model as normal graph model at user defined folder, for example `.\\exported-models\\infer_graph` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068a60a-de97-4c49-ad0f-9c28d68b0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "python export_inference_graph.py --pipeline_config_path models/my_ssd_mobilenet_v3/pipeline.config --trained_checkpoint_prefix models/my_ssd_mobilenet_v3/model.ckpt-1004 --output_directory exported-models/inference_graph_tflite --add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab24521-1687-4f9a-8202-1259888064f8",
   "metadata": {},
   "source": [
    "# Convert to tflite\n",
    "---\n",
    "- Please update your `source_graph_model_folder` and `output_tflite_location`.\n",
    "- `dynamic_quant_enable` is dynamic quantization with 8-bit weights and activations. The model size will smaller, but the performance maybe worse.\n",
    "- Please directly excute the next block.\n",
    "- <pre> training_demo/\n",
    "├─ ...\n",
    "├─ exported-models/\n",
    "│  └─ inference_graph_tflite/\n",
    "│     ├─ tflite_graph.pb\n",
    "│     ├─ tflite_graph.pbtxt   \n",
    "│     └─ mobilenetv3_ssd_v1.tflite (the output file after excuting below)\n",
    "└─ ...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c797d8-f16b-46f4-861d-a6e65f108836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_graph_model_folder = \"training_demo_tf1/exported-models/inference_graph_tflite/tflite_graph.pb\"\n",
    "#output_tflite_location = \"training_demo_tf1/exported-models/inference_graph_tflite/mobilenetv3_ssd_v1.tflite\"\n",
    "source_graph_model_folder = \"training_demo_tf1/exported-models/inference_graph_tflite/tflite_graph.pb\"\n",
    "output_tflite_location = \"training_demo_tf1/exported-models/inference_graph_tflite/mobilenetv3_ssd_f16.tflite\"\n",
    "rep_dataset_loc = r\"C:\\Users\\USERNAME\\image_detection\\image_dataset\\COCO\\images\\val2017\\*.jpg\"\n",
    "dynamic_quant_enable = False\n",
    "float16_quant = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1926618-f339-4d03-866b-0e130721d21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1976484"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_arrays = [\"normalized_input_image_tensor\"]\n",
    "output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\n",
    "#output_arrays = [\"detection_boxes\", \"detection_classes\", \"detection_scores\", \"num_boxes\"]\n",
    "#input_arrays = [\"serving_default_input:0\"]\n",
    "#output_arrays = ['StatefulPartitionedCall','StatefulPartitionedCall:1','StatefulPartitionedCall:2','StatefulPartitionedCall:3']\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
    "  source_graph_model_folder, \n",
    "  input_arrays, \n",
    "  output_arrays, \n",
    "  input_shapes={'normalized_input_image_tensor':[1, 320, 320, 3]}\n",
    "  )\n",
    "converter.allow_custom_ops = True\n",
    "if dynamic_quant_enable or float16_quant:\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if float16_quant:   \n",
    "    converter.target_spec.supported_types = [tf.float16]    \n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open(output_tflite_location, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386ef7d",
   "metadata": {},
   "source": [
    "# Convert to int8 tflite\n",
    "- We need tensorflow2 to convert to int8 tflite\n",
    "- So please run this section in TF-2 python env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "\n",
    "class my_tflite_trans():\n",
    "    def __init__(self,source_model_folder, output_tflite_location, rep_dataset_loc):\n",
    "        self.source_model_folder = source_model_folder\n",
    "        self.output_tflite_location = output_tflite_location\n",
    "        self.rep_dataset_loc = rep_dataset_loc\n",
    "\n",
    "    def tflite_preprocess(self, image, height, width):\n",
    "        if image.dtype != tf.float32:\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    \n",
    "        # Resize the image to the specified height and width.\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.compat.v1.image.resize_bilinear(image, [height, width],\n",
    "                                       align_corners=False)\n",
    "        #image = tf.squeeze(image, [0])\n",
    "    \n",
    "        image = tf.subtract(image, 0.5)\n",
    "        image = tf.multiply(image, 2.0)\n",
    "        return image\n",
    "    \n",
    "    def representative_dataset(self):\n",
    "        files = glob(self.rep_dataset_loc)\n",
    "        random.shuffle(files)\n",
    "        files = files[:256]\n",
    "        for file in files:\n",
    "            #print(file)\n",
    "            image = tf.io.read_file(file)\n",
    "            image = tf.compat.v1.image.decode_jpeg(image)\n",
    "            if image.get_shape()[2] == 3: # skip the not correct channel pictures\n",
    "                image = self.tflite_preprocess(image, 320, 320)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            yield [image]\n",
    "    \n",
    "    #def representative_dataset(self):\n",
    "    #    for _ in range(100):\n",
    "    #        data = np.random.rand(1, 320, 320, 3)\n",
    "    #        yield [data.astype(np.float32)]\n",
    "\n",
    "    def run_tflite(self):\n",
    "        \n",
    "        input_arrays = [\"normalized_input_image_tensor\"]\n",
    "        output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\n",
    "        converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
    "                    self.source_model_folder, \n",
    "                    input_arrays, \n",
    "                    output_arrays, \n",
    "                    input_shapes={'normalized_input_image_tensor':[1, 320, 320, 3]}\n",
    "                    )\n",
    "        # Refer to: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md#step-2-convert-to-tflite\n",
    "        #converter = tf.lite.TFLiteConverter.from_saved_model(self.source_model_folder)\n",
    "        \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "        converter.representative_dataset = self.representative_dataset\n",
    "        converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "        #converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "        \n",
    "        converter.allow_custom_ops = True\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        #del(converter)\n",
    "        #del(representative_dataset)\n",
    "        #gc.collect()\n",
    "        \n",
    "        # remove the original file\n",
    "        #try:\n",
    "        #    os.remove(self.output_tflite_location)\n",
    "        #except OSError as e:\n",
    "        #    print(e)\n",
    "        \n",
    "        # Save the model.\n",
    "        with open(self.output_tflite_location, 'wb') as f:\n",
    "            f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_tflite_trans(source_graph_model_folder, output_tflite_location, rep_dataset_loc)\n",
    "x.run_tflite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee47fe-42ca-451e-9b9c-b8163c665bd7",
   "metadata": {},
   "source": [
    "- The input shape can be changed as follow commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f05c6-47d5-46bb-909d-6242dd8c2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_convert --graph_def_file=training_demo_tf1\\exported-models\\tf12tf2\\tflite_graph.pb --output_file=training_demo_tf1\\exported-models\\tf12tf2\\ssd_mobilenetv3_1126.tflite --input_arrays='normalized_input_image_tensor' --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --input_shape=1,320,320,3 --allow_custom_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18973247-c8b2-4d64-9f05-1579bf3f8e57",
   "metadata": {},
   "source": [
    "<a id=\"id-IH\"></a>\n",
    "# Issue Help\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a1abe-24fc-42c2-ba10-77db7cb5f891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
