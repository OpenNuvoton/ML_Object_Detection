{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7764ee9-5abf-48d7-865b-30d10f1fe7c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This notebook will help you go through the below steps (1). model setting (2). training (3). exporting tflite\n",
    "---\n",
    "- The `training_demo` is user defined folder name under `workspace`. You can check `image_dataset\\create_data.ipynb` for how to create your own training folder.\n",
    "- In this notebook step, you have alreay finish the dataset prepared. If not, please go to `image_dataset\\create_data.ipynb`.\n",
    "- It is recommended to copy the cmds below and use CMD, PowerShell or terminal outside this notebook.\n",
    "- All the commands below are needed excuted under `workspace\\training_demo`.\n",
    "- \\<Advanced>: The more detail is in this link [tensorflow-object-detection-api-tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f248ab-8c0d-4dca-9411-ad6e5676264f",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa05feb-96e9-4fb7-8622-930a8f8df3bd",
   "metadata": {},
   "source": [
    "## Download Pre-Trained Model\n",
    "- The model in this examples is the `SSD MobileNet V2 FPNLite 320x320`\n",
    "- All of the tensorflow2 pre-trained models are listed in [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), and you can choose other models and download it.\n",
    "- The download file is `*.tar.gz`, and please decompression it (e.g. 7zip, WinZIP, etc.).\n",
    "- Move `ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8` inside the folder `training_demo/pre-trained-models`\n",
    "\n",
    "- <pre>training_demo/\n",
    "├─ ...\n",
    "├─ pre-trained-models/\n",
    "│  └─ ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/\n",
    "│     ├─ checkpoint/\n",
    "│     ├─ saved_model/\n",
    "│     └─ pipeline.config\n",
    "└─ ...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d06f6-3e5e-48f0-91d3-8a9552447d1b",
   "metadata": {},
   "source": [
    "## Configure the Training Pipeline\n",
    "- The parameters below is basing on your files/folders nameing. Please update them if any change.\n",
    "    1. `training_dir`: The folder name of user defined working directory\n",
    "    2. `my_model_directory_name`: The file location of user defined which save the training weights, checkpoints and *.config\n",
    "    3. `fine_tune_checkpoint`: The file location of user downloaded pre-trained-models checkpoint\n",
    "    4. `train_record_fname`: The file location of user created tfrecord for training\n",
    "    5. `test_record_fname`: The file location of user created tfrecord for testing\n",
    "    6. `label_map_pbtxt_fname`: The file location of label map\n",
    "    7. `batch_size`: Increase/Decrease this value depending on the available memory\n",
    "    8. `num_steps`: How many the training steps.  \n",
    "- Please excute the below 2 blocks.\n",
    "- This is for `SSD MobileNet V2 FPNLite 320x320` pipeline.config, if you use other model, the pipeline.config maybe have minor different. However, these attributes should be the same and mattered.\n",
    "- \\<Advanced>: If you want to tunning more parameters, please update `pipeline.config` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b962f4-5f91-491d-87c0-0c2e2a4a051f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dir = 'training_demo'\n",
    "\n",
    "my_model_directory_name = 'models/my_ssd_mobilenet_v2_fpnlite'\n",
    "fine_tune_checkpoint = 'pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0' \n",
    "train_record_fname = 'annotations/train.record' \n",
    "test_record_fname = 'annotations/test.record' \n",
    "label_map_pbtxt_fname = \"annotations/label_map.pbtxt\"\n",
    "batch_size = 16 \n",
    "num_steps = 50000\n",
    "image_resizer = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c3364e-611d-4056-9be1-93e08ea3a2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERNAME\\image_detection\\install test\\ML_tf2_object_detection_nu\\workspace\\training_demo\\annotations/label_map.pbtxt\n",
      "The number of class: 1\n",
      "writing custom configuration file...\n",
      "The train config file is at: C:\\Users\\USERNAME\\image_detection\\install test\\ML_tf2_object_detection_nu\\workspace\\training_demo\\models/my_ssd_mobilenet_v2_fpnlite\\pipeline.config\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import regex as re\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "home_path = os.getcwd() \n",
    "path_para_list = [my_model_directory_name, fine_tune_checkpoint, train_record_fname, test_record_fname, label_map_pbtxt_fname]\n",
    "update_path_para_list = list(map(lambda x : os.path.join(home_path, training_dir, x), path_para_list))  #update the\n",
    "\n",
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())\n",
    "#print(update_path_para_list[4])\n",
    "num_classes = get_num_classes(update_path_para_list[4])\n",
    "print('The number of class: {}'.format(num_classes))\n",
    "\n",
    "def create_user_folder(dir_path):\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "        print('skip create...')\n",
    "def copy_user_file(src, dst):\n",
    "    try:\n",
    "        shutil.copy(src, dst)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "def update_config(src_fld, dst_fld):\n",
    "    print('writing custom configuration file...')\n",
    "\n",
    "    with open(os.path.join(src_fld, 'pipeline.config')) as f:\n",
    "        s = f.read()\n",
    "    print('The train config file is at: {}'.format(os.path.join(dst_fld, 'pipeline.config')))    \n",
    "    with open(os.path.join(dst_fld, 'pipeline.config'), 'w') as f:\n",
    "        \n",
    "        # fine_tune_checkpoint\n",
    "        s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                   'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "        # label_map_path\n",
    "        s = re.sub(\n",
    "            'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "        # Set training batch_size.\n",
    "        s = re.sub('batch_size: [0-9]+',\n",
    "                   'batch_size: {}'.format(batch_size), s)\n",
    "        # Set training steps, num_steps\n",
    "        s = re.sub('num_steps: [0-9]+',\n",
    "                   'num_steps: {}'.format(num_steps), s)\n",
    "        # Set number of classes num_classes.\n",
    "        s = re.sub('num_classes: [0-9]+',\n",
    "                   'num_classes: {}'.format(num_classes), s)\n",
    "        # Set number of fixed_shape_resizer.\n",
    "        s = re.sub('height: [0-9]+',\n",
    "                   'height: {}'.format(image_resizer), s)\n",
    "        s = re.sub('width: [0-9]+',\n",
    "                   'width: {}'.format(image_resizer), s)\n",
    "        #fine-tune checkpoint type\n",
    "        s = re.sub(\n",
    "            'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "        \n",
    "        # tfrecord files train and test. (the train section must before test section)\n",
    "        s = re.sub(\n",
    "            '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s, 1)\n",
    "        s = re.sub(\n",
    "            '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s, 1)\n",
    "        \n",
    "        f.write(s)            \n",
    "# create model_directory            \n",
    "create_user_folder(update_path_para_list[0])\n",
    "# copy pipeline.config\n",
    "update_config(update_path_para_list[1].split(r'checkpoint')[-2], update_path_para_list[0])\n",
    "#copy_user_file(os.path.join(update_path_para_list[1].split(r'checkpoint')[-2], 'pipeline.config'), update_path_para_list[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edaaf9b-544d-4f4e-af8f-97fdac1097a1",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "- Please open CMD.exe Prompt or PowerShell Prompt and `cd` inside your working folder, for example, `training_demo` folder\n",
    "    - for example: `cd ML_tf2_object_detection_nu\\workspace\\training_demo`\n",
    "- Next step is copy the below command and paste it on the CMD or PowerShell.exe. Please remember to update the path & folder path.    \n",
    "    - `--model_dir` is user defined folder which is user defined my_model_directory_name. The training processes and variables are saved in here.\n",
    "    - `--pipeline_config_path` is the location of user defined pipeline.config.\n",
    "- <pre> training_demo/\n",
    "├─ ...\n",
    "├─ models/\n",
    "│  └─ my_ssd_mobilenet_v2_fpnlite/\n",
    "│     └─ pipeline.config\n",
    "└─ ...\n",
    "- The output will normally look like it has “frozen”, but DO NOT rush to cancel the process. The training outputs logs only every 100 steps by default, therefore if you wait for a while, you should see a log for the loss at step 100.\n",
    "</pre>\n",
    "- <img src=\"train_exmple_plots/train_process.png\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04304c0f-f422-4c54-a67e-2ab5c0a8a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python model_main_tf2.py --model_dir=models/my_ssd_mobilenet_v2_fpnlite --pipeline_config_path=models/my_ssd_mobilenet_v2_fpnlite/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b526619-caef-4e12-b7dd-6ca8a246afd4",
   "metadata": {},
   "source": [
    "# Evaluating the Model (Optional)\n",
    "---\n",
    "- Please open another CMD.exe Prompt or PowerShell Prompt to run the command when running the train step.\n",
    "- The operation step is same as training the model, and you need inside your working folder to excute the command.\n",
    "    - `--checkpoint_dir` is the location of each training save point, and it is saved in models folder.\n",
    "- If there is an error, please check [help](#id-IH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ba88c-41a2-4b8b-b32b-247692b80870",
   "metadata": {},
   "outputs": [],
   "source": [
    "python model_main_tf2.py --model_dir=models/my_ssd_mobilenet_v2_fpnlite --pipeline_config_path=models/my_ssd_mobilenet_v2_fpnlite/pipeline.config --checkpoint_dir=models/my_ssd_mobilenet_v2_fpnlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30aa668-6853-4bcb-8a82-4505204f203a",
   "metadata": {},
   "source": [
    "# Monitor Training Job Progress using TensorBoard\n",
    "---\n",
    "- Please open another CMD.exe Prompt or PowerShell Prompt to run the command when running the train step.\n",
    "- You need inside your working folder to excute the command.\n",
    "    - `--logdir` is the location of each training save point, and it is saved in models folder.\n",
    "- Copy the URL and paste it on browser (except IE) as below:\n",
    "- <img src=\"train_exmple_plots/tf_board_url.png\" width=\"400\" height=\"300\">\n",
    "- The board is as below:\n",
    "- <img src=\"train_exmple_plots/tf_board.png\" width=\"400\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cff18-289c-463c-a40a-9f1c1189e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=models/my_ssd_mobilenet_v2_fpnlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826940b8-72c0-4a2d-93b9-a9ad0a7988e3",
   "metadata": {},
   "source": [
    "# Export a TFLite inference graph\n",
    "---\n",
    "- To deploy on edge device, we should use this command (output TFLite inference graph).\n",
    "- Please open another CMD.exe Prompt or PowerShell Prompt to run the command.\n",
    "- The operation step is same as training the model, and you need inside your working folder to excute the command.\n",
    "    - `--pipeline_config_path` is the location of user defined pipeline.config.\n",
    "    - `--trained_checkpoint_dir` is the location of each training save point, and it is saved in models folder.\n",
    "    - `--output_directory` is the user defined folder to save your output model graph, for example `tflite_infer_graph_XX`. In this way, it is easy to distinguish different model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba1dd1-1d86-41eb-b8eb-253918b9b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "python export_tflite_graph_tf2.py --pipeline_config_path .\\models\\my_ssd_mobilenet_v2_fpnlite\\pipeline.config --trained_checkpoint_dir .\\models\\my_ssd_mobilenet_v2_fpnlite --output_directory .\\exported-models\\tflite_infer_graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1910e2b-fbb5-42cd-a2e4-fedb2b752eb6",
   "metadata": {},
   "source": [
    "# Exporting a Trained Model graph (Optional)\n",
    "---\n",
    "- Output the model as normal graph model.\n",
    "- The operation step is same as Tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e94a47-185a-406a-8892-8bd6aed0d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python exporter_main_v2.py --input_type image_tensor --pipeline_config_path .\\models\\my_ssd_mobilenet_v2_fpnlite\\pipeline.config --trained_checkpoint_dir .\\models\\my_ssd_mobilenet_v2_fpnlite\\ --output_directory .\\exported-models\\infer_graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab24521-1687-4f9a-8202-1259888064f8",
   "metadata": {},
   "source": [
    "# Convert to tflite\n",
    "---\n",
    "- This is final step to convert to tflite file.\n",
    "- Please update your `source_model_folder` and `output_tflite_location` basing on your working folder.\n",
    "- `dynamic_quant_enable` is dynamic quantization with 8-bit weights and activations. The model size will smaller, but the performance maybe worse.\n",
    "- Please directly excute the next 3 blocks.\n",
    "- <pre> training_demo/\n",
    "├─ ...\n",
    "├─ exported-models/\n",
    "│  └─ tflite_infer_graph/\n",
    "│     ├─ saved_model/\n",
    "│        └─ saved_model.pb\n",
    "│     └─ mobilev2_ssd.tflite (the output file after excuting below)\n",
    "└─ ...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f0003a-1d49-4233-90bb-9c497346e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model_folder = 'training_demo/exported-models/tflite_infer_graph/saved_model'\n",
    "output_tflite_location = 'training_demo/exported-models/tflite_infer_graph/mobilev2_ssd_fullquant.tflite'\n",
    "dynamic_quant_enable = False\n",
    "\n",
    "full_quant = True\n",
    "\n",
    "#rep_dataset_loc = \"C:/Users/USERNAME/image_detection/TensorFlow/workspace/training_demo_8000/images/test/*.jpg\"  ##This is the place of imgs location\n",
    "rep_dataset_loc = r\"C:\\Users\\USERNAME\\image_detection\\image_dataset\\COCO\\images\\val2017\\*.jpg\"\n",
    "float16_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a555a9-3366-407c-990d-80f2cd7eef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "\n",
    "class my_tflite_trans():\n",
    "    def __init__(self,source_model_folder, output_tflite_location, dynamic_quant_enable, full_quant, rep_dataset_loc, float16_quant):\n",
    "        self.source_model_folder = source_model_folder\n",
    "        self.output_tflite_location = output_tflite_location\n",
    "        self.dynamic_quant_enable = dynamic_quant_enable\n",
    "        self.full_quant = full_quant\n",
    "        self.rep_dataset_loc = rep_dataset_loc\n",
    "        self.float16_quant = float16_quant\n",
    "\n",
    "    def tflite_preprocess(self, image, height, width):\n",
    "        if image.dtype != tf.float32:\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    \n",
    "        # Resize the image to the specified height and width.\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.compat.v1.image.resize_bilinear(image, [height, width],\n",
    "                                       align_corners=False)\n",
    "        #image = tf.squeeze(image, [0])\n",
    "    \n",
    "        image = tf.subtract(image, 0.5)\n",
    "        image = tf.multiply(image, 2.0)\n",
    "        return image\n",
    "    \n",
    "    def representative_dataset(self):\n",
    "        files = glob(self.rep_dataset_loc)\n",
    "        random.shuffle(files)\n",
    "        files = files[:256]\n",
    "        for file in files:\n",
    "            #print(file)\n",
    "            image = tf.io.read_file(file)\n",
    "            image = tf.compat.v1.image.decode_jpeg(image)\n",
    "            if image.get_shape()[2] == 3: # skip the not correct channel pictures\n",
    "                image = self.tflite_preprocess(image, 320, 320)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            yield [image]\n",
    "    \n",
    "    #def representative_dataset(self):\n",
    "    #    for _ in range(100):\n",
    "    #        data = np.random.rand(1, 320, 320, 3)\n",
    "    #        yield [data.astype(np.float32)]\n",
    "\n",
    "    def run_tflite(self):\n",
    "        # Refer to: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md#step-2-convert-to-tflite\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(self.source_model_folder)\n",
    "        \n",
    "        if self.dynamic_quant_enable:\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        if self.full_quant:\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "            converter.representative_dataset = self.representative_dataset\n",
    "            converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "            #converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "        \n",
    "        if self.float16_quant: \n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            converter.target_spec.supported_types = [tf.float16]\n",
    "            converter.representative_dataset = self.representative_dataset\n",
    "        \n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        #del(converter)\n",
    "        #del(representative_dataset)\n",
    "        #gc.collect()\n",
    "        \n",
    "        # remove the original file\n",
    "        #try:\n",
    "        #    os.remove(self.output_tflite_location)\n",
    "        #except OSError as e:\n",
    "        #    print(e)\n",
    "        \n",
    "        # Save the model.\n",
    "        with open(self.output_tflite_location, 'wb') as f:\n",
    "            f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8014d5b8-edf7-4e82-8fb5-8787f17a8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_tflite_trans(source_model_folder, output_tflite_location, dynamic_quant_enable, full_quant, rep_dataset_loc, float16_quant)\n",
    "x.run_tflite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18973247-c8b2-4d64-9f05-1579bf3f8e57",
   "metadata": {},
   "source": [
    "<a id=\"id-IH\"></a>\n",
    "# Issue Help\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed499d6-6508-499d-95bc-cf6db8ef622e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
